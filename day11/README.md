# DAY 11 â€” TRANSFORMER ARCHITECTURE
### Understanding Attention, Multi-Head Attention, Positional Encoding & Encoder Blocks

---

## ğŸš€ Overview
Day 11 introduces the **Transformer Architecture**, the foundation of modern LLMs such as GPT, LLaMA, Gemini and Claude.  
You will understand how transformers replace RNNs using **self-attention**, enabling parallelism and long-range context learning.

---

## ğŸ¯ Learning Goals
- Understand the math behind self-attention  
- Learn Queries, Keys, Values  
- Implement scaled dot-product attention  
- Implement multi-head attention  
- Add sinusoidal positional encoding  
- Build a transformer encoder block in PyTorch  
- Run a full forward pass & inspect shapes  

---

## ğŸ“‚ Folder Structure
```
day11
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ theory.md
â”œâ”€â”€ tutorial.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”‚
â””â”€â”€ code/
    â”œâ”€â”€ model_components.py
    â”œâ”€â”€ utils.py
    â”œâ”€â”€ example_inputs.py
    â””â”€â”€ demo.py
```

---

## ğŸ”¥ Hands-On Tasks
- Build Scaled Dot-Product Attention  
- Build Multi-Head Attention  
- Implement Positional Encoding  
- Build Transformer Encoder Block  
- Run `demo.py` to test a forward pass  

---

## â–¶ï¸ How to Run
```bash
cd day11-transformer
pip install -r requirements.txt
python code/demo.py
```

---

## ğŸ“š Resources
- The Illustrated Transformer â€” Jay Alammar  
- Attention Is All You Need â€” Paper  
- PyTorch Transformer Docs  
- 3Blue1Brown â€” Attention Mechanism  

---