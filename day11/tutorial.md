# Day 11 â€” Hands-On Tutorial

## ğŸ¯ Objective
Implement the core components of a Transformer Encoder Block and run a full forward pass to deeply understand how self-attention works in practice.

---

## ğŸ§  Step-by-Step Tasks

### 1ï¸âƒ£ Implement Scaled Dot-Product Attention  
File to edit: `code/model_components.py`

Key goals:
- Compute QKáµ€  
- Scale by âˆšdâ‚–  
- Apply softmax  
- Multiply with V  
- Understand attention weights  

---

### 2ï¸âƒ£ Implement Multi-Head Attention  
Key concepts:
- Split embedding dimension into `num_heads`  
- Reshape tensors correctly  
- Parallel attention across heads  
- Concatenate and project output  

---

### 3ï¸âƒ£ Implement Positional Encoding  
File: `code/utils.py`

Why?
Transformers do not understand order â€” positional encodings add sequence information.

You will:
- Implement sinusoidal encodings  
- Add them to token embeddings  

---

### 4ï¸âƒ£ Build Transformer Encoder Block  
Components:
- Multi-Head Attention  
- Add & LayerNorm  
- Feed Forward Network  
- Add & LayerNorm  

This is the core transformer layer used in LLMs.

---

### 5ï¸âƒ£ Run Demo  
File: `code/demo.py`

The demo performs:
- Random input generation  
- Positional encoding  
- Forward pass through encoder  
- Prints shapes:  
  - input  
  - output  
  - attention weights  

This confirms your implementation is correct.

---

## ğŸ“ Recommended Experiments
Try the following to deepen understanding:

- Change number of heads (e.g., 2 â†’ 4 â†’ 8)  
- Change sequence length (8 â†’ 16 â†’ 32)  
- Visualize attention weights  
- Add multiple encoder layers  
- Try different feed-forward dimensions (`d_ff = 128, 256, 512`)  

---

## ğŸ“Œ Learning Outcomes
By completing this tutorial, you will:
- Understand the math behind attention  
- Know how multi-head attention is implemented  
- Understand positional encodings in detail  
- Build and run a Transformer Encoder Block  
- Gain confidence to explore decoder blocks, BERT, GPT, etc.  

---

## ğŸš€ Next Steps
After Day 11, you can:
- Implement a small Transformer encoder classifier  
- Implement a GPT-style decoder block  
- Learn about RNN vs Transformer performance differences  

